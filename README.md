# WebRTC FastAPI Video Chat

A real-time peer-to-peer video chat application built with FastAPI and WebRTC.

## Features

- ğŸ¥ Real-time video and audio communication
- ğŸ”’ Room-based private conversations
- ğŸŒ WebSocket signaling server
- ğŸ“± Responsive design
- ğŸ›ï¸ Media controls (mute/unmute, video on/off)
- âºï¸ Recording with client-side speech-to-text
- ğŸ”„ Live recording indicator and shared transcript across the room
- ğŸ”’ Single active recorder per room (others see status and transcript)
- ğŸ§  LLM summaries (OpenAI or Hugging Face)
- â¬‡ï¸ Export summary to PDF
- ğŸ‘¥ Multiple participants support
- ğŸ³ Docker support

## Quick Start with Docker

### Prerequisites

- Docker and Docker Compose installed
- A webcam and microphone
- Modern web browser

### Run the Application

```bash
# Build and start
docker-compose up -d

# View logs
docker-compose logs -f

# Stop
docker-compose down
```

Access: http://localhost:8000 (or see [Access from Other Devices](#access-from-other-devices))

## Manual Setup (without Docker)

### 1. Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Run the Application

```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

## Usage

1. Open http://localhost:8000 in your browser
2. Allow camera and microphone permissions
3. Enter a room name
4. Click "Join Room"
5. Share the room name with others
6. Start video chatting!

### Record + Transcribe Speech

- Click "âºï¸ Start Recording" to begin transcribing spoken audio (Chrome-based browsers).
- Click again to stop; the transcript saves to the database.
- Everyone in the same room sees:
  - A live "Recording" indicator with who is recording
  - The evolving transcript in real time
  - Final transcript text appended when speech segments finish

### Summarize + Export

- Click "ğŸ§  Summarize" to generate a concise summary of the room conversation using your configured LLM provider.
- Click "â¬‡ï¸ Download PDF" to download a wellâ€‘structured PDF (title, room, timestamp, content).
- API endpoints:
  - POST `/summaries/room/{room}` â†’ `{ room, summary }`
  - POST `/summaries/room/{room}/pdf` â†’ `application/pdf`

## Access from Other Devices

WebRTC permissions require a **secure context** (HTTPS or localhost). When you visit `http://<your-ip>:8000` from another machine, browsers block camera/mic access. Use one of the following approaches.


### Option A: Secure Tunnel with ngrok

If you prefer not to manage certificates, you can proxy the app through an HTTPS tunnel:

```bash
# Terminal 1
run application

# Terminal 2 (requires an ngrok account and auth token)
ngrok http 8000
```

Use the `https://` forwarding URL generated by ngrok from any device. Camera/mic prompts work because ngrok terminates TLS for you.

## Testing Locally

Open two browser tabs:
- Tab 1: Enter room "test-room" and join
- Tab 2: Enter same room "test-room" and join
- Both tabs should connect and show video

## Docker Commands

```bash
# Build
docker-compose build

# Start
docker-compose up -d

# Stop
docker-compose down

# View logs
docker-compose logs -f

# Restart
docker-compose restart

# Remove everything
docker-compose down -v
```

## Project Structure

```
webrtc-fastapi/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”œâ”€â”€ config.py            # Centralized settings from .env
â”‚   â”œâ”€â”€ db.py                # SQLAlchemy engine/session
â”‚   â”œâ”€â”€ db_models.py         # ORM models (e.g., Transcript)
â”‚   â”œâ”€â”€ schemas.py           # Pydantic schemas
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ webrtc.py        # WebSocket signaling
â”‚   â”‚   â”œâ”€â”€ transcripts.py   # REST: store/list transcripts
â”‚   â”‚   â””â”€â”€ summaries.py     # REST: summarize + PDF
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ llm/
â”‚   â”‚       â”œâ”€â”€ base.py      # LLMProvider interface
â”‚   â”‚       â”œâ”€â”€ factory.py   # Provider selector (OpenAI/HF)
â”‚   â”‚       â”œâ”€â”€ openai_provider.py
â”‚   â”‚       â””â”€â”€ hf_provider.py
â”‚   â””â”€â”€ static/
â”‚       â”œâ”€â”€ css/
â”‚       â”‚   â””â”€â”€ style.css
â”‚       â”œâ”€â”€ js/
â”‚       â”‚   â””â”€â”€ webrtc.js
â”‚       â””â”€â”€ index.html
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Configuration

Edit `.env` file:

```
STUN_SERVER=stun:stun.l.google.com:19302
HOST=0.0.0.0
PORT=8000
DATABASE_URL=postgresql+psycopg2://webrtc:webrtc@localhost:5432/webrtc

Using local Postgres (no DB container):
- Ensure your local PostgreSQL is running and accessible on port 5432
- Update credentials in `.env` and `docker-compose.yml` as needed
- For Docker on macOS/Windows, use `host.docker.internal` as the host in `DATABASE_URL`
  Example: `postgresql+psycopg2://webrtc:webrtc@host.docker.internal:5432/webrtc`
- On Linux, use the host IP or set up Docker host networking.

Docker Compose mounts your local `.env` into the container (see `docker-compose.yml`) so the app reads environment variables directly from it.

LLM config (choose one provider):
- OpenAI
  - `LLM_PROVIDER=openai`
  - `OPENAI_API_KEY=sk-...`
  - Optional: `OPENAI_MODEL=gpt-4o-mini`
- Hugging Face
  - `LLM_PROVIDER=huggingface`
  - `HF_API_KEY=hf_...`
  - Optional: `HF_MODEL=facebook/bart-large-cnn`

APIs:
- POST `/transcripts` { room, client_id, language?, text } â†’ stores a transcript row
- POST `/summaries/room/{room}` â†’ returns `{ summary }` using configured LLM
- POST `/summaries/room/{room}/pdf` â†’ returns `application/pdf`
- GET  `/transcripts?room=<name>&limit=<N>` â†’ recent transcript rows
```

## Troubleshooting

### Camera not working
- Check browser permissions
- Use HTTPS (`mkcert` or `ngrok` when testing off-device)
- Try different browser

### Port already in use
```bash
# Use different port
docker-compose up -d -e PORT=8001
```

### Connection failed
- Ensure both users in same room
- Check firewall settings
- View logs: `docker-compose logs -f`

### LLM summary errors
- Error: "LLM provider not configured" â†’ set `LLM_PROVIDER` and the corresponding API key in `.env`.
- Running in Docker: ensure `.env` is mounted (it is by default) and restart after editing.
- OpenAI httpx error â†’ we pin `httpx` to `0.27.x` in `requirements.txt` for compatibility.

### Transcripts not saving
- Only the active recorder saves rows; ensure your device is the recorder.
- DATABASE_URL must point to a reachable Postgres host from inside the container.
- Tables autoâ€‘create on startup; ensure the app started successfully.

## Production Deployment

For production use:
1. Use HTTPS (WebRTC requires secure context)
2. Add TURN servers for NAT traversal
3. Implement authentication
4. Set up monitoring

## License

MIT License

## Support

For issues, please check the logs and browser console (F12).


build:
	docker-compose build

up:
	docker-compose up -d
	@echo "âœ… Application started at http://localhost:8000"

dev:
	docker-compose up

down:
	docker-compose down

restart:
	docker-compose restart

logs:
	docker-compose logs -f

clean:
	docker-compose down -v
	docker system prune -f

shell:
	docker-compose exec webrtc-app /bin/bash
